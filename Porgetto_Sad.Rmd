---
title: "Report SAD"
author: "Genito Leone"
date: "2023-01-09"
output:
  pdf_document: default
  html_document: default
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Dati sull'efficienza del carburante dal 1999 al 2008 per 38 modelli di auto popolari

## Descrizione

Il dataset "mpg" (Miles per Gallon) fornito con il pacchetto
**`ggplot2`** in R contiene informazioni sull'efficienza del carburante
di diverse automobili. Il dataset include le seguenti variabili:

-   **`manufacturer`**: il produttore dell'automobile.

-   **`model`**: il modello dell'automobile.

-   **`displ`**: il volume dei cilindri dell'automobile, in litri.

-   **`year`**: l'anno di produzione dell'automobile.

-   **`cyl`**: il numero di cilindri dell'automobile.

-   **`trans`**: il tipo di trasmissione dell'automobile.

-   **`drv`**: il tipo di trazione dell'automobile (anteriore,
    posteriore o integrale).

-   **`cty`**: il consumo di carburante in città, in miglia per gallone.

-   **`hwy`**: il consumo di carburante in autostrada, in miglia per
    gallone.

-   **`fl`**: il tipo di carburante utilizzato dall'automobile.

-   **`class`**: la classe di dimensioni dell'automobile.

Il dataset "mpg" è composto da 234 osservazioni, una per ogni automobile
presente nel dataset. Utilizziamo questo dataset per esplorare la
relazione tra l'efficienza del carburante e diverse caratteristiche
delle automobili, come il produttore, il modello, il volume dei cilindri
o il tipo di trasmissione.

# Esplorazione dei dati

## Installazione del pacchetto ggplot2

Il primo passo è installare il pacchetto **`ggplot2`** utilizzando il
comando **`install.packages("ggplot2")`** e quindi caricarlo in R
utilizzando il comando **`library(ggplot2)`**.

## Caricamento delle librerie

```{r include=FALSE}
library(ggplot2)
library(plotly)
library(dplyr)
library(tidyverse)
library(caret)
library(stats)
library(cluster)
library(GGally)
```

## Import del dataset

Eseguiamo il comando **`data(mpg)`** per caricare il dataset "mpg" in
memoria.

```{r}
data(mpg)
```

## Panoramica del dataset

Utilizziamo il comando **`head(mpg)`** per visualizzare le prime
osservazioni del dataset. Questo ci darà un'idea di come sono
strutturati i dati e delle variabili incluse nel dataset.

```{r}
head(df)
```

Il comando **`head()`** è utile per esplorare i dati e avere una
panoramica delle osservazioni presenti nel dataset. Vogliamo
visualizzare anche le ultime osservazioni del dataset, quindi
utilizziamo il comando **`tail(mpg)`**.

```{r}
tail(mpg)
```

Vogliamo visualizzare un numero specifico di osservazioni, quindi
utilizziamo il comando **`head(mpg, n)`**, dove **`n`** è il numero di
osservazioni che vuoi visualizzare.

```{r}
head(mpg, 10)
```

## Pulizia dei dati

La pulizia dei dati consiste nel processo di verifica e correzione dei
dati contenuti in un dataset al fine di garantirne la qualità e la
validità per un'analisi statistica. La pulizia dei dati può includere
diverse attività, come:

-   Rimozione dei valori mancanti: si verifica la presenza di valori
    mancanti (NA) nel dataset e si sceglie come gestirli, ad esempio
    eliminandoli o sostituendoli con valori sostitutivi.

-   Correzione dei valori errati: si verifica la presenza di valori
    errati o anomali nel dataset e si sceglie come gestirli, ad esempio
    eliminandoli o sostituendoli con valori sostitutivi.

-   Trasformazione delle variabili: si verifica il tipo di dati per ogni
    variabile (ad esempio, numerico, carattere, data) e si effettua
    eventualmente la trasformazione.

-   Creazione di nuove variabili: si creano nuove variabili derivate
    dalle variabili esistenti, ad esempio calcolando il rapporto tra due
    variabili o creando una variabile binaria a partire da una variabile
    categoriale.

### Verifica della presenza di valori mancanti

Verificare la presenza di NA nel dataset, dunque procediamo con il
comando **`sum(is.na(mpg))`** per contare il numero di valori mancanti.

```{r}
sum(is.na(mpg))
```

Non sono presenti valori nulli nel data frame. In caso contrario avremmo
potuto scegliere di sostituirli con la media o la moda, nel caso fossero
stati valori numerici.

Agli altri step non è dedicato nessun paragrafo in quanto non serve
affrontarli. Non ci sono valori errati, come sarà possibile vedere nel
summary mostrato successivamente nel report. Non c'è bisogno di
effettuare nessuna trasformazione di tipo in quanto ognuno è coerente.
Non vediamo il bisogno di creare nuove variabili.

## Descrizione del dataset

Utilizziamo il comando **`str(mpg)`** per visualizzare una descrizione
delle variabili del dataset. Questo ci mostrerà il tipo di dati di ogni
variabile (ad esempio, numerico o carattere) e il numero di osservazioni
presenti per ogni variabile.

```{r}
str(mpg)
```

Il comando **`str()`** è utile per ottenere informazioni sulla struttura
del dataset e sulla tipologia di dati contenuti in ogni variabile. Ad
esempio, dalla descrizione delle variabili del dataset "mpg" mostrata
sopra, possiamo vedere che la variabile **`manufacturer`** è di tipo
carattere (**`chr`**), ovvero una stringa di testo, mentre la variabile
**`year`** è di tipo intero (**`int`**), ovvero un numero intero.

In R, il tipo numerico (**`numeric`**), come la variabile **`displ`**,
rappresenta i numeri con la virgola mobile, ovvero i numeri con
decimale. Ad esempio, 3.141592 è un numero numerico.

Il tipo intero (**`integer`**) rappresenta invece i numeri interi, lo
sono ad esempio **`year`** **`cyl`** ,ovvero i numeri senza decimale. Ad
esempio, 3 è un numero intero.

## Panoramica delle variabili

Utilizzando il comando **`summary(mpg)`** otteniamo una panoramica delle
statistiche descrittive per ogni variabile del dataset.

```{r}
summary(mpg)
```

La funzione **`summary()`** calcola automaticamente alcune statistiche
di base per ogni variabile, come ad esempio il valore minimo, il valore
massimo, la media, la mediana e la deviazione standard. Se la variabile
è di tipo numerico, verranno calcolate anche il quartile e il range
interquartile. Se la variabile è di tipo carattere o factor, verrà
conteggiato il numero di osservazioni per ogni livello o valore univoco
della variabile. La funzione **`summary()`** è utile per ottenere una
panoramica delle caratteristiche dei dati contenuti in un dataset. Ad
esempio, dall'output della funzione **`summary()`** per il dataset "mpg"
possiamo vedere il numero di osservazioni per ogni variabile (ad
esempio, "Length:234" indica che ci sono 234 osservazioni per ogni
variabile), il tipo di dati contenuti in ogni variabile (ad esempio,
"Class :character" indica che la variabile è di tipo carattere) e alcune
statistiche di base per ogni variabile (ad esempio, "Min. :1.600" indica
il valore minimo della variabile, "1st Qu.:2.400" indica il primo
quartile, "Median :3.300" indica la mediana e così via).

### Visualizzazione rapida dei grafici

Visualizziamo i grafici delle variabili numeriche del dataset mpg

```{r}
dfm <- mpg %>% 
  select_if(is.numeric)
```

Ora che abbiamo raccolto tutte le variabili di tipo numerico, guardiamo
il grafico ottenuto con pairs

```{r}
pairs(dfm)
```

Dal grafico notiamo che c'è una correlazione negativa tra le variabili
displ e cty e displ e hwy, ossia tra il volume dei cilindri
dell'automobile e il consumo di carburante in città ed in autostrada.
Quindi maggiore è il volume dei cilindri e minore è il consumo di
carburante. Una forte correlazione positiva vi è anche tra il consumo di
carburante in strada ed in autostrada.

## Tabelle di frequenza

Utilizziamo il comando **`table(mpg$variabile)`** per ottenere il
conteggio delle osservazioni per ogni categoria di una variabile. Ad
esempio, **`table(mpg$cyl)`** ti fornirà il conteggio delle osservazioni
per il numero di cilindri per ogni automobile.

La funzione **`table()`** è utile anche per ottenere una panoramica
delle caratteristiche di una variabile e per individuare eventuali
problemi o incongruenze nei dati. Ad esempio, se una variabile contiene
molti valori mancanti o valori anomali, questi verranno segnalati nella
tabella di frequenze.

### Manufacturer

La variabile manufacturer del dataset mpg rappresenta il produttore del
veicolo. Si tratta di una variabile categorica nominale, in quanto i
valori possibili sono limitati e non possono essere ordinati in base
alla loro posizione nella scala di valori. I valori possibili per la
variabile manufacturer sono 15 categorie diverse, ad esempio "audi",
"chevrolet", "dodge" e così via.

```{r}
df <- data.frame(Freq = c(table(mpg$manufacturer)), Perc_freq = c(table(mpg$manufacturer)/length(mpg$manufacturer)*100))
df

```

La variabile "Manufacturer" è di tipo carattere o factor e contiene
diversi nomi di produttori (ad esempio, "Ford", "Toyota", "Nissan"), la
tabella di frequenze mostrerà il numero di osservazioni per ogni
produttore.

### Model

La variabile model del dataset mpg rappresenta il modello del veicolo.
Si tratta di una variabile categoriale, in quanto i valori possibili
sono discreti e limitati.

```{r}
df <- data.frame(Freq = c(table(mpg$model)), Perc_freq = c(table(mpg$model)/length(mpg$model)*100))
df

```

Toyota tacome 4wd è il modello di veicolo più frequente in questo
dataset mentre il modello 4runner 4wd è il meno diffuso.

Del modello caravan 2wd ne sono presenti 11 veicoli dunque il 5% mentre
il modello a4 costituisce il 3% del set di dati.

### Displ

La variabile displ del dataset mpg rappresenta la cilindrata del motore
del veicolo, ovvero il volume delle camere di combustione, misurato in
pollici cubici (in). Si tratta di una variabile quantitativa continua.

```{r}
df <- data.frame(Freq =
c(table(mpg$displ)), Perc_freq = c(table(mpg$displ)/length(mpg$displ)*100))
df

```

Tutti i valori presenti nella tabella in alto rappresentano la
cilindrata del motore dei veicoli. 7 è il valore della cilindrata più
alto e ne è caratterizzato solo un veicolo ovvero solo 0.5%. La
cilindrata più bassa è 1.6 e 5 veicoli ne sono dotati ovvero il 2%
dell'intero set di dati.

### Year

Year indica l'anno di produzione dell'automobile. La variabile year del
dataset mpg rappresenta l'anno di produzione del veicolo. Si tratta di
una variabile quantitativa discreta, in quanto i valori possibili sono
limitati e non possono assumere valori intermedi.

```{r}
df <- data.frame(Freq =
c(table(mpg$year)), Perc_freq = c(table(mpg$year)/length(mpg$year)*100))
df

```

Dalla tabella di frequenza in alto possiamo notare che il 50% dei
veicoli è stato prodotto nel 1999 mentre l'altro 50% nel 2008.

### Cyl

La variabile cyl del dataset mpg rappresenta il numero di cilindri del
motore del veicolo. Si tratta di una variabile quantitativa discreta, in
quanto i valori possibili sono limitati e non possono assumere valori
intermedi.

```{r}
df <- data.frame(Freq =
c(table(mpg$cyl)), Perc_freq = c(table(mpg$cyl)/length(mpg$cyl)*100))
df

```

I valori elencati sono il numero di cilindri del motore dei veicoli,
possiamo notare che il 35% dei veicoli ha un numero di cilindri pari a
4, 70 auto circa il 30% ha un motore a 8 cilindri, 4 veicoli sono dotati
di un motore a 5 cilindri ed infine il 34% delle auto ha un motore a 6
cilindri.

### Trans

La variabile trans del dataset mpg rappresenta il tipo di trasmissione
del veicolo. Si tratta di una variabile categorica ordinale, in quanto i
valori possibili sono limitati e possono essere ordinati in base alla
loro posizione nella scala di valori.

```{r}
df <- data.frame(Freq =
c(table(mpg$trans)), Perc_freq = c(table(mpg$trans)/length(mpg$trans)*100))
df

```

I valori presenti in questa tabella rappresentano il tipo di
trasmissione dei veicoli: 83 veicoli ovvero 36% del dataset hanno il
tipo di trasmissione auto(l4), circa il 25% sono caratterizzati dalla
trasmissione manual(m5), 39 auto invece dalla trasmissione auto(l5).

### Drv

La variabile drv del dataset mpg rappresenta il tipo di trazione del
veicolo. Si tratta di una variabile categorica nominale, in quanto i
valori possibili sono limitati e non possono essere ordinati in base
alla loro posizione nella scala di valori. I valori possibili per la
variabile drv sono "f", "r" e "4", che indicano rispettivamente trazione
anteriore, trazione posteriore e trazione integrale.

```{r}
 df <- data.frame(Freq =
c(table(mpg$drv)), Perc_freq = c(table(mpg$drv)/length(mpg$drv)*100))
df

```

103 auto dunque il 44% possiedono la trazione integrale, il 45% dunque
106 auto hanno la trazione anteriore infine 25 veicoli il 10% sono
caratterizzati dalla trazione posteriore.

### Cty

La variabile cty del dataset mpg rappresenta il numero di miglia per
gallone che un veicolo può percorrere in città. Si tratta di una
variabile quantitativa continua, in quanto i valori possibili sono
numeri reali che possono assumere valori intermedi.

```{r}
df <- data.frame(Freq =
c(table(mpg$cty)), Perc_freq = c(table(mpg$cty)/length(mpg$cty)*100))
df

```

35 è il numero massimo di miglia per gallone che 1 auto del nostro
dataset può percorrere in città, il numero minimo di miglia per gallone
che 5 auto (2%) del nostro set di dati possono fare in città è 9.

### Hwy

La variabile hwy del dataset mpg rappresenta il numero di miglia per
gallone che un veicolo può percorrere su strada. Si tratta di una
variabile quantitativa continua, in quanto i valori possibili sono
numeri reali che possono assumere valori intermedi.

```{r}
df <- data.frame(Freq =
c(table(mpg$hwy)), Perc_freq = c(table(mpg$hwy)/length(mpg$hwy)*100))
df
```

2 auto del dataset dunque l'1% riescono a percorrere 44 miglia per
gallone su strada, il 2% ovvero 12 auto coprono solo 12 miglia per
gallone su strada e cosi via.

### Fl

La variabile fl del dataset mpg rappresenta il tipo di carburante
utilizzato dal veicolo. Si tratta di una variabile categorica nominale,
in quanto i valori possibili sono limitati e non possono essere ordinati
in base alla loro posizione nella scala di valori. I valori possibili
per la variabile fl sono "p", "c", "e", "r" e "d", che indicano
rispettivamente: carburante premium, carburante normale (etanolo 85),
elettricità, carburante regolare e diesel.

```{r}
df <- data.frame(Freq =
c(table(mpg$fl)), Perc_freq = c(table(mpg$fl)/length(mpg$fl)*100)) 
df

```

Dalla tabella di frequenza possiamo notare che 1 auto usa il carburante
normale per viaggiare, il gasolio premium lo usano il 22% circa di
veicoli dunque 52 auto, il carburante regolare è quello più comune
infatti 168 auto ovvero 168 veicoli ne fanno uso infine 8 auto si
riforniscono con l'elettricità e 5 veicoli con il diesel.

### Class

La variabile class del dataset mpg rappresenta la classe di veicolo a
cui appartiene il veicolo. Si tratta di una variabile categorica
nominale, in quanto i valori possibili sono limitati e non possono
essere ordinati in base alla loro posizione nella scala di valori. I
valori possibili per la variabile class sono 19 categorie diverse, ad
esempio "suv", "pickup", "minivan" e così via.

```{r}
df <- data.frame(Freq =
c(table(mpg$class)), Perc_freq = c(table(mpg$class)/length(mpg$class)*100))
df

```

La classe di veicolo più frequente è suv infatti di questa classe sono
presenti 62 auto dunque il 25%, 5 veicoli circa il 2% appartiene alla
classe di veicolo 2seater e così via.

## Visualizzazione dei dati

Visualizzare i dati attraverso grafici serve a diverse finalità:

1.  Facilitare la comprensione dei dati: i grafici possono aiutare a
    rappresentare in modo visivo i dati e a renderli più facili da
    comprendere. Ad esempio, un grafico a barre può mostrare facilmente
    le frequenze di una variabile categoriale, mentre un grafico a linee
    può mostrare l'evoluzione di una variabile nel tempo.

2.  Individuare pattern e tendenze: i grafici possono aiutare a
    individuare pattern e tendenze nei dati, che altrimenti potrebbero
    essere difficili da rilevare dall'analisi dei dati in forma
    tabellare.

3.  Fare confronti e raffronti: i grafici possono consentire di fare
    confronti e raffronti tra diverse categorie o gruppi di dati, ad
    esempio confrontando le frequenze di diverse variabili.

4.  Comunicare i risultati: i grafici possono essere utilizzati per
    comunicare i risultati di un'analisi a un pubblico più ampio, anche
    a persone che non sono esperte di statistica o di analisi dei dati.
    Ad esempio, un grafico può essere incluso in un report o in una
    presentazione, come in questo caso, per illustrare in modo chiaro e
    conciso i risultati di un'analisi.

### Variabile manufacturer

Per visualizzare la variabile "manufacturer" del dataset "mpg" con
ggplot2, utilizziamo un grafico a barre e un grafico a torta. Entrambi
questi grafici sono adatti per visualizzare le frequenze di una
variabile di tipo carattere o factor, come la variabile "manufacturer".

#### Grafico a Barre

Il grafico a barre è un tipo di grafico utilizzato per visualizzare le
frequenze di una variabile di tipo carattere o factor. In particolare,
ogni barra del grafico rappresenta il numero di osservazioni per un
determinato livello o valore univoco della variabile.

```{r message=FALSE, warning=FALSE}
p1 <- ggplot(mpg, aes(x = manufacturer)) +
     geom_bar(color = "black", fill = "white") + theme(axis.text.x = element_text(angle = 90)) + ylab("Manufacturer")

p1 <- p1 + theme(panel.background = element_rect(fill = '#f0e2a8', color = '#9a8262'), 
               panel.grid.major = element_line(color = '#ffffff', linetype = 'longdash'),
               panel.grid.minor = element_line(color = '#fff68f', size = 0.7,linetype = "dotdash")) 

p1
```

Il comando ha prodotto un grafico a barre che visualizza il numero di
osservazioni per ogni produttore di auto presente nella variabile
"manufacturer".

Il grafico a barre è utile per ottenere una panoramica delle
caratteristiche di una variabile e per individuare eventuali tendenze o
pattern nei dati. Ad esempio, se un produttore di auto è presente in
molti più casi rispetto agli altri, questo potrebbe indicare che è più
popolare o diffuso.

#### Grafico a Torta

Il grafico a torta (o grafico a settori) è un tipo di grafico utilizzato
per visualizzare le frequenze di una variabile di tipo carattere o
factor. In particolare, ogni settore della torta rappresenta la
percentuale di osservazioni per un determinato livello o valore univoco
della variabile.

```{r}
Manufacturer <- factor(mpg$manufacturer, levels = names(sort(table(mpg$manufacturer))))
p2 <- ggplot(mpg, aes(x = "", y = manufacturer, fill = Manufacturer)) +
  geom_bar(width = 1, stat = "identity") +
  coord_polar("y") + theme_void()

p2 <- p2 + theme(panel.background = element_rect(fill = '#f0e2a8', color = '#9a8262'), 
               panel.grid.major = element_line(color = '#f0e2a8', linetype = 'longdash'),
               panel.grid.minor = element_line(color = '#fff68f', size = 0.7,linetype = "dotdash")) 

p2

```

Il grafico a torta mostra la distribuzione delle osservazioni per ogni
produttore di auto presente nella variabile 'manufacturer' del dataset
'mpg'.

LE PERCENTUALI SONO SBAGLIATE

l grafico mostra che il produttore 'Toyota' è presente in circa il 20%
delle osservazioni, seguito da 'Dodge' con il 15% e 'Ford' con il 10%.

VEDI SE RIESCI AD INFILARCI LA LEGGE DI PARETO perché il 20% dei marchi
hanno l'80% del mercato

### Variabile model

Anche in questo caso essendo la variabile categoriale procediamo con la
rappresentazione attraverso un grafico a barre, mentre non verrà
mostrato il grafico a torta in quanto è irrilevante, la presenza di così
tante categorie non permetterebbe di distinguere l'una dall'altra.

#### Grafico a barre

```{r}
p3 <- ggplot(mpg, aes(x = model, fill = Manufacturer)) +
  geom_bar(position = "dodge")+ theme(axis.text.x = element_text(angle = 90))+ ylab("Manufacturer")

p3 <- p3 + theme(panel.background = element_rect(fill = '#f0e2a8', color = '#9a8262'), 
               panel.grid.major = element_line(color = '#ffffff', linetype = 'longdash'),
               panel.grid.minor = element_line(color = '#fff68f', size = 0.7,linetype = "dotdash")) 

p3


```

Il grafico a barre della variabile "model" condizionato a "manufacturer"
mostra la distribuzione delle frequenze dei modelli di auto per ogni
produttore. Ad esempio, possiamo vedere quanti modelli di auto di ogni
produttore sono presenti nel dataset "mpg". Il grafico può essere utile
per ottenere una panoramica della presenza di ogni produttore
all'interno del dataset e per comparare le frequenze dei modelli di auto
tra i diversi produttori. Con questo grafico sappiamo ogni modello a
quale casa produttrice appartiene e conosciamo anche quanti di quei
modelli sono presenti nel dataset.

### Variabile displ

La variabile displ rappresenta il volume dei cilindri dell'automobile,
in litri. Per visualizzare i dati della variabile "displ" del dataset
"mpg", utilizziamo un istogramma.

#### Istogramma

Gli istogrammi sono formati da bins, ossia le barre verticali che
dividono l'asse delle ascisse in intervalli di valori. Ogni barra
rappresenta un bin, ovvero un intervallo di valori, e l'altezza della
barra indica il numero di osservazioni che cadono all'interno di
quell'intervallo.

Per scegliere il numero ottimale di bins abbiamo utilizzato la regola di
Sturges, questa ci suggerisce di utilizzare il numero di bins pari a
**`log2(n) + 1`**, dove **`n`** è il numero di osservazioni.

```{r}
n_bins_sturges <- function(x) {
  log2(length(x)) + 1
}
```

L'istogramma è un grafico che mostra la distribuzione delle frequenze di
una variabile numerica. È utile per ottenere una panoramica delle
caratteristiche della variabile, come la forma della distribuzione, il
valore medio ecc.

```{r message=FALSE, warning=FALSE}
p4 <- ggplot(mpg, aes(x = displ)) + geom_histogram(bins = 30, aes(y = ..density..), color = "black", fill = "white") + 
  geom_freqpoly(aes(y = ..density..), color = "red", size = 1) + labs(title = "Distribuzione dei valori di displ") 


p4 <- p4 + theme(panel.background = element_rect(fill = '#f0e2a8', color = '#9a8262'), 
               panel.grid.major = element_line(color = '#f0e2a8', linetype = 'longdash'),
               panel.grid.minor = element_line(color = '#fff68f', size = 0.7,linetype = "dotdash")) 

p4
```

L'istogramma della variabile "displ" mostra la distribuzione dei valori
della variabile. Ad esempio, possiamo vedere qual è la frequenza di ogni
valore di "displ" all'interno del dataset "mpg". Il grafico visualizza i
valori di "displ" come valori in ascissa e le frequenze come valori in
ordinata.

La curva di frequenza mostra la frequenza cumulativa dei valori di
"displ", ossia la somma delle frequenze di una variabile fino a un
determinato valore.

Si può notare che la maggior parte dei valori di "displ" sono compresi
tra 1 e 4, con un picco intorno a 2. Ciò indica che la distribuzione dei
valori è sbilanciata a sinistra, con una maggiore concentrazione di
valori a sinistra della media.

#### Grafico a barre

Vogliamo confrontare i valori della variabile "disp" tra le diverse
categorie della variabile "class". Per farlo utilizziamo un grafico a
barre con i valori di "disp" come valori in ascissa e le categorie di
"class" come valori in ordinata:

```{r}
macchina <- factor(mpg$class, ordered = T)
p5 <- ggplot(mpg, aes(x = macchina, y = displ)) +
  geom_bar(stat = "identity", fill = "white") + ylab("displ") + xlab("Class")


p5 <- p5 + theme(panel.background = element_rect(fill = '#f0e2a8', color = '#9a8262'), 
               panel.grid.major = element_line(color = '#f0e2a8', linetype = 'longdash'),
               panel.grid.minor = element_line(color = '#fff68f', size = 0.7,linetype = "dotdash")) 

p5
```

Dal grafico possiamo osservare sulle ascisse i tipi di veicoli.

#### Grafico a dispersione

Questo grafico mostra la relazione tra due variabili numeriche.
Utilizziamo il grafico a dispersione per visualizzare come i valori di
"displ" sono correlati ai valori di un'altra variabile numerica, come
"cty" (l consumo di carburante in città, in miglia per gallone).

```{r}
p6 <- ggplot(mpg, aes(x = displ, y = cty)) +
  geom_point()

p6 <- p6 + theme(panel.background = element_rect(fill = '#f0e2a8', color = '#9a8262'), 
               panel.grid.major = element_line(color = '#fff68f', linetype = 'longdash'),
               panel.grid.minor = element_line(color = '#fff68f', size = 0.7,linetype = "dotdash")) 

p6
```

Sull'asse delle ascisse ci sono le cilindrate del motore, ossia il
numero di camere di combustione, sull'asse delle ordinate c'è il consumo
di carburante in città, in miglia per gallone

Dal grafico vediamo che all'aumentare del numero di cilindri diminuisce
il consumo di carburante in città.

### Variabile Year

Year indica l'anno di produzione dell'automobile. Innanzitutto
applichiamo la funzione **`summary()`** per ottenere un riassunto delle
statistiche di base per la variabile **`Year`**, come ad esempio la
media, la mediana, il minimo e il massimo.

```{r}
summary(mpg$year)

```

L'auto più vecchia del dataset è del 1999, mentre la più recente è del
2008. Questo ci fa capire che le auto prese in esame sono modelli molto
vecchi rispetto a quelli attuali.

#### Istogramma

Nel chunk successivo applichiamo la funzione **`geom_histogram`**,
l'equivalente di **`hist()`**, per creare un istogramma della variabile
**`Year`**. L'istogramma ci aiuterà a visualizzare la distribuzione dei
valori della variabile e a identificare eventuali outlier.

```{r}
p7 <- ggplot(mpg, aes(x = year)) + geom_histogram(bins = 30, aes(y = ..density..), color = "black", fill = "white")
p7 <- p7 + theme(panel.background = element_rect(fill = '#f0e2a8', color = '#9a8262'), 
               panel.grid.major = element_line(color = '#fff68f', linetype = 'longdash'),
               panel.grid.minor = element_line(color = '#fff68f', size = 0.7,linetype = "dotdash")) 

p7
```

Da questo grafico capiamo che gli anni di produzione sono solo 2, il
1999 e il 2008. Quindi sono presenti due generazioni di veicoli, alcuni
prodotti alla fine del 20esimo secolo e altri all'inizio del 21 secolo,
discostanti da loro di 10 anni.

#### Boxplot

Con la funzione **`boxplot()`** creiamo un grafico a scatola a baffi
della variabile **`Year`**.

```{r}
p8 <- ggplot(mpg, aes(x = year)) + geom_boxplot() + coord_flip()
p8 <- p8 + theme(panel.background = element_rect(fill = '#f0e2a8', color = '#9a8262'), 
               panel.grid.major = element_line(color = '#fff68f', linetype = 'longdash'),
               panel.grid.minor = element_line(color = '#fff68f', size = 0.7,linetype = "dotdash")) 

p8
```

Il boxplot in questo caso appare anomalo, perché non ha i baffi, questo
perché il primo quartile corrisponde al minimo e il terzo quartile
corrisponde al massimo.

### Variabile Cyl

La variabile **`cyl`** del dataset **`mpg`** rappresenta il numero di
cilindri del motore dei veicoli. Si tratta di una variabile categoriale,
in quanto i valori possibili sono discreti e limitati.

#### Barplot

Nel seguente chunk abbiamo creato un grafico a barre per visualizzare la
distribuzione dei valori.

```{r}
p9 <- ggplot(data = mpg, mapping = aes(cyl)) + geom_bar(stat = "count", fill= "white", col = "black") + ggtitle("Variabile cyl") + ylab("Cilindri")

p9 <- p9 + theme(panel.background = element_rect(fill = '#f0e2a8', color = "#9a8262"), panel.grid.major = element_line(color = '#ffffff', linetype="longdash"),panel.grid.minor = element_line(color = '#fff68f', size = 0.7,linetype = "dotdash"))

p9
```

Nel grafico, "cyl" sarà mostrata sull'asse delle ascisse e il numero di
osservazioni per ogni valore sarà mostrato sull'asse delle ordinate.

Il grafico a barre mostra che ci sono molte più macchine con un numero
di cilindri pari a 4 rispetto a vetture con un numero di cilindri pari a
5 o 8.

#### Grafico a torta

Utilizziamo un grafico a torta per mostrare le frequenze di ogni valore
di "cyl" come percentuali dell'intero dataset.

Per avere un grafico a torta corretto, bisogna prima trasformare la
variabile cyl in un fattore e riordina i valori in base alla frequenza,

```{r}
Cilindri <- factor(mpg$cyl, levels = names(sort(table(mpg$cyl))))
```

In questo modo, i valori di "Cilindri" verranno ordinati in base alla
frequenza, dal valore più frequente al meno frequente

```{r}

p10 <- ggplot(mpg, aes(x = "", y = Cilindri, fill = Cilindri)) +
  geom_bar(width = 1,stat = "identity") +
  coord_polar("y") + theme_void()

p10 <- p10 + theme(panel.background = element_rect(fill = '#f0e2a8', color = '#9a8262'), 
               panel.grid.major = element_line(color = '#f0e2a8', linetype = 'longdash'),
               panel.grid.minor = element_line(color = '#fff68f', size = 0.7,linetype = "dotdash")) 

p10

```

Il grafico a torta ci da una conferma di quello che avevamo già visto
nel grafico a barre, la presenza di veicoli a 4 cilindri è superiore
alle altre. In sostanza il dataset è formato quasi interamente da
veicoli a 4 e 6 cilindri.

### Variabile trans

La variabile **`trans`** del dataset **`mpg`** rappresenta il tipo di
trasmissione dei veicoli. Si tratta di una variabile categoriale, in
quanto i valori possibili sono discreti e limitati.

#### Barplot

```{r}
p11 <- ggplot(mpg, mapping = aes(trans)) +
  geom_bar(fill = "white", col = "black") + ylab("Trasmissione")

p11 <- p11 + theme(panel.background = element_rect(fill = '#f0e2a8', color = '#9a8262'), 
               panel.grid.major = element_line(color = '#f0e2a8', linetype = 'longdash'),
               panel.grid.minor = element_line(color = '#fff68f', size = 0.7,linetype = "dotdash")) 

p11

```

Dall'istogramma possiamo notare che ci sono molti veicoli con il tipo di
trasmissione *auto(l4)* e il tipo di trasmissione *manual(m5)* .

#### Boxplot

```{r}
p12 <- ggplot(mpg, aes(x = trans, y = displ)) +
  geom_boxplot()

p12 <- p12 + theme(panel.background = element_rect(fill = '#f0e2a8', color = '#9a8262'), 
               panel.grid.major = element_line(color = '#f0e2a8', linetype = 'longdash'),
               panel.grid.minor = element_line(color = '#fff68f', size = 0.7,linetype = "dotdash")) 

p12
```

Il boxplot è un grafico utile per visualizzare la distribuzione di una
variabile continua stratificata per una variabile discreta, poiché
permette di individuare facilmente le differenze tra le distribuzioni
delle diverse categorie. Ad esempio, in questo caso, il grafico mostra
come la distribuzione dei valori della variabile **`displ`** cambi in
base al tipo di trasmissione del veicolo. Possiamo notare ad esempio che
la mediana dei valori della variabile **`displ`** per il tipo di
trasmissione "automatic" è leggermente più alta rispetto al tipo di
trasmissione "manual". Oppure, che ci sono più outliers per il tipo di
trasmissione "automatic" rispetto al tipo di trasmissione "manual".

### Variabile Drv

La variabile Drv rappresenta il tipo di trazione dell'automobile
(anteriore, posteriore o integrale).

#### Barplot

```{r}
drive <- factor(mpg$drv, levels = c("4", "f", "r"), labels = c("trazione integrale", "trazione anteriore", "trazione posteriore"))
p13 <- ggplot(mapping = aes(drive)) + 
  geom_bar(fill = "white", col = "black") + ylab("Trazione")

p13 <- p13 + theme(panel.background = element_rect(fill = '#f0e2a8', color = '#9a8262'), 
               panel.grid.major = element_line(color = '#f0e2a8', linetype = 'longdash'),
               panel.grid.minor = element_line(color = '#fff68f', size = 0.7,linetype = "dotdash")) 

p13

```

La maggior parte dei veicoli possiede la trazione integrale e la
trazione anteriore, sono pochi i veicoli con la trazione posteriore.

#### Scatterplot

```{r}
p14 <- ggplot(mpg, aes(x = drive, y = displ)) +
  geom_point()

p14 <- p14 + theme(panel.background = element_rect(fill = '#f0e2a8', color = '#9a8262'), 
               panel.grid.major = element_line(color = '#f0e2a8', linetype = 'longdash'),
               panel.grid.minor = element_line(color = '#fff68f', size = 0.7,linetype = "dotdash")) 

p14
```

Dallo scatterplot possiamo osservare che i veicoli con la trazione
integrale, possono avere qualsiasi volume dei cilindri (in litri). I
veicoli con la trazione anteriore invece possiedono la cilindrata del
motore bassa a differenza delle auto con la trazione posteriore che sono
caratterizzate da una cilindrata alta.

Giungiamo alla medesima conclusione osservando il boxplot in basso.

#### Boxplot

```{r}
p15 <- ggplot(mpg, aes(x = drv, y = displ)) +
  geom_boxplot() + coord_flip()

p15 <- p15 + theme(panel.background = element_rect(fill = '#f0e2a8', color = '#9a8262'), 
               panel.grid.major = element_line(color = '#f0e2a8', linetype = 'longdash'),
               panel.grid.minor = element_line(color = '#fff68f', size = 0.7,linetype = "dotdash")) 

p15
```

La centralità delle scatole, espressa dalla mediana vediamo che non è
propriamente al centro, ciò sta a significare che non c'è simmetria nei
dati, piuttosto la distribuzione è asimmetrica, come nel caso del primo
boxplot che è fortemente asimmetrica a sinistra. Dall'ultimo boxplot
notiamo dalla lunghezza dei baffi che c'è molta dispersione nei dati.
Mentre nel secondo boxplot vediamo che c'è un outliers, ossia un valore
anomalo che va molto al di fuori dei baffi.

### Variabile Cty

La variabile Cty rappresenta il consumo di carburante in città, in
miglia per gallone.

#### Istogramma e kernel density

```{r}
p16 <- ggplot(data = mpg, mapping = aes(cty,..density..)) + 
  geom_histogram(bins = 30, fill = "white", col = "black")+
  geom_density()

p16 <- p16 + theme(panel.background = element_rect(fill = '#f0e2a8', color = '#9a8262'), 
               panel.grid.major = element_line(color = '#f0e2a8', linetype = 'longdash'),
               panel.grid.minor = element_line(color = '#fff68f', size = 0.7,linetype = "dotdash")) 

p16
```

Dall'istogramma possiamo osservare come si distribuisce la variabile, è
evidente che la maggior parte dei veicoli percorrono tra i 10 e i 20
miglia con un gallone.

#### Scatterplot

```{r}
p17 <- ggplot(data = mpg, mapping = aes(x = cty, y = displ)) +
  geom_point()

p17 <- p17 + theme(panel.background = element_rect(fill = '#f0e2a8', color = '#9a8262'), 
               panel.grid.major = element_line(color = '#f0e2a8', linetype = 'longdash'),
               panel.grid.minor = element_line(color = '#fff68f', size = 0.7,linetype = "dotdash")) 

p17

```

Le variabili displ (volume dei cilindri dell'automobile, in litri) e cty
(consumo di carburante in città, in miglia per gallone) sono correlate
negativamente, infatti all'aumentare delle miglia che il veicolo può
percorrere con un gallone diminuisce la cilindrata del motore dell'auto.

```{r}
p18 <- ggplot(mpg, aes(x = cyl, y = cty)) +
  geom_point()

p18 <- p18 + theme(panel.background = element_rect(fill = '#f0e2a8', color = '#9a8262'), 
               panel.grid.major = element_line(color = '#f0e2a8', linetype = 'longdash'),
               panel.grid.minor = element_line(color = '#fff68f', size = 0.7,linetype = "dotdash")) 

p18
```

Osservando lo scatter plot possiamo affermare che le auto con un numero
di cilindri pari a *4 e 5* riescono a percorrere più miglia con un
gallone in città mentre le auto con *6* e *8* cilindri riescono a
percorrere meno miglia per gallone in zona urbana.

#### Istogramma

Utilizziamo la funzione **`cut()`** per raggruppare i dati e creare un
istogramma con degli intervalli scelti.

```{r}
fcut <- table (cut ( mpg$cty, breaks = c(12,14,17,20,35) ))
fcut
```

Gli intervalli sono scelti in modo da essere abbastanza equi.

```{r}
barplot(fcut, col = rainbow(4), xlab = "miglia per gallone", main = "Grafico miglia per gallone", ylab = "n.osservazioni")
```

### Variabile Hwy

La variabile hwy del dataset mpg rappresenta il numero di miglia per
gallone che un veicolo può percorrere su strada.

Usando le funzioni di visualizzazione come **`ggplot()`** del pacchetto
**`ggplot2`** è possibile creare grafici che mostrino le relazioni tra
le variabili del dataset o la distribuzione di singole variabili. Ad
esempio, abbiamo creato un istogramma per visualizzare i consumi di
carburante **`hwy`** dei veicoli del dataset.

#### Istogramma

```{r}
p19 <- ggplot(data = mpg, mapping = aes(hwy)) +
  geom_histogram(bins = 30, fill = "white", col = "black") 

p19 <- p19 + theme(panel.background = element_rect(fill = '#f0e2a8', color = '#9a8262'), 
               panel.grid.major = element_line(color = '#f0e2a8', linetype = 'longdash'),
               panel.grid.minor = element_line(color = '#fff68f', size = 0.7,linetype = "dotdash")) 

p19

```

#### Kernel density plot

Il grafico kernel density plot è un grafico che mostra l'approssimazione
della distribuzione dei valori di una variabile continuamente misurata.
È ottenuto tramite l'utilizzo di una tecnica di smoothing nota come
"kernel density estimation", che consiste nell'aggiungere una curva di
densità di kernel sopra ogni osservazione. La curva di densità di kernel
è una funzione continua che rappresenta la distribuzione di probabilità
dei valori di una variabile.

Il grafico kernel density plot è spesso utilizzato per esplorare la
distribuzione di una variabile e per confrontare le distribuzioni di
diverse variabili. Ad esempio, abbiamo utilizzato il grafico kernel
density plot per confrontare la distribuzione dei valori della variabile
**`hwy`**.

```{r}
p20 <- ggplot(data = mpg, mapping = aes(hwy)) +
  geom_density(fill = "white", col = "black")

p20 <- p20 + theme(panel.background = element_rect(fill = '#f0e2a8', color = '#9a8262'), 
               panel.grid.major = element_line(color = '#f0e2a8', linetype = 'longdash'),
               panel.grid.minor = element_line(color = '#fff68f', size = 0.7,linetype = "dotdash")) 

p20

```

La maggior parte dei veicoli presenti in questo dataset percorrono su
strada tra i 20 e i 30 miglia con un gallone.

#### Grafico a dispersione

```{r}
p21 <- ggplot(data = mpg, mapping = aes(x = hwy, y = displ)) +
  geom_point()

p21 <- p21 + theme(panel.background = element_rect(fill = '#f0e2a8', color = '#9a8262'), 
               panel.grid.major = element_line(color = '#f0e2a8', linetype = 'longdash'),
               panel.grid.minor = element_line(color = '#fff68f', size = 0.7,linetype = "dotdash")) 

p21

```

Dallo scatterplot è evidente che la variabile *hwy* (consumo di
carburante in autostrada, in miglia per gallone) è correlata
negativamente con la variabile *displ* (cilindrata del motore,ovvero il
volume delle camere di combustione, misurato in pollici cubici (in)),
possiamo affermare infatti che all'aumentare delle miglia che il veicolo
può percorrere su strada, con un gallone, diminuisce la cilindrata del
motore, dunque più alta è la cilindrata e minori sono le miglia che il
veicolo percorrerà con un gallone su strada.

### Variabile Fl

La variabile fl del dataset mpg rappresenta il tipo di carburante
utilizzato dal veicolo. I valori possibili per la variabile fl sono "p",
"c", "e", "r" e "d", che indicano rispettivamente: carburante premium,
carburante normale (etanolo 85), elettricità, carburante regolare e
diesel.

#### Grafico a barre

```{r}
fuel <- factor(mpg$fl, levels = c("c","d","e","p","r"), labels = c("Normale gasoline", "Diesel", "Elettricity","Premium gasoline", "Regular gasoline")) 

p22 <- ggplot(data = mpg, mapping = aes(x = fuel)) +
  geom_bar(col = "black", fill = "white") + ylab("Fuel")

p22 <- p22 + theme(panel.background = element_rect(fill = '#f0e2a8', color = '#9a8262'), 
               panel.grid.major = element_line(color = '#f0e2a8', linetype = 'longdash'),
               panel.grid.minor = element_line(color = '#fff68f', size = 0.7,linetype = "dotdash")) 

p22

```

Possiamo notare che la maggiore parte delle auto presenti nel set di
riferimento, circa il 72%, usa il gasolio regolare per viaggiare, il 22%
si rifornisce con il carburante premium ed il restante 6% delle auto con
gli altri combustibili: elettricità, diesel o carburante normale.

#### Grafico frequenze relative cumulative

Le frequenze assolute cumulative sono il numero di osservazioni di una
variabile che sono inferiori o uguali a un determinato valore
In R, utilizziamo la funzione **`cumsum()`** per calcolare le frequenze
assolute cumulative di una variabile.

```{r}
fac <- cumsum(table(mpg$fl)/length(mpg$fl))
names(fac) <- c("Normal_gasoline", "Diesel", "Elettricity", "Premium_Gasoline", "Regular gas")

plot(fac, type = "s", main = "FRC Carburante", ylim = c(-0.5,1.3), xlab= "Tipo di carburante")
```

Il grafico a gradini mostra il numero di automobili con trasmissione
"fl" che sono presenti nel dataset e che hanno una trasmissione di tipo
uguale o inferiore a quello indicato sull'ascissa. Ad esempio, il primo
segmento orizzontale del grafico rappresenta il numero di automobili con
trasmissione "manual", mentre il secondo segmento orizzontale
rappresenta il numero di automobili con trasmissione "manual" o "auto".

#### Grafico a torta

```{r}
freqRel <- table (mpg$fl )/ length (mpg$fl)
perc <- freqRel*100
perc <- round(perc,1)
LabelF <- c("Normal_gas", "Diesel", "Elettricity", "Premium_Gas", "Regular_gas") 
LabelF <- paste(LabelF, perc)
LabelF <- paste(LabelF, "%", sep = "")
pie ( perc , label = LabelF , col = rainbow ( length ( LabelF )), main = "Valori percentuali ")
```

### Variabile Class

La variabile Class rappresenta la classe di dimensioni dell'automobile.

#### Barplot

```{r}
p23 <- ggplot(data = mpg, mapping = aes(class)) + 
  geom_bar(fill = "white", col = "black") + ylab("Fuel")

p23 <- p23 + theme(panel.background = element_rect(fill = '#f0e2a8', color = '#9a8262'), 
               panel.grid.major = element_line(color = '#f0e2a8', linetype = 'longdash'),
               panel.grid.minor = element_line(color = '#fff68f', size = 0.7,linetype = "dotdash")) 

p23
```

Dall'istogramma possiamo notare che ci sono molti veicoli *suv* nel
nostro dataset, l'altra classe di veicoli molto frequente è *compact* e
*midsize*

# Modello di regressione lineare

Il dataset "mpg" fornito con il pacchetto **`ggplot2`** in R potrebbe
essere adeguato per eseguire un'analisi di regressione lineare multipla,
a seconda della domanda di ricerca o dell'ipotesi che vuoi verificare.
La regressione lineare multipla è un modello statistico che permette di
indagare le relazioni tra più variabili indipendenti (chiamate anche
"predictor") e una variabile dipendente (chiamata anche "risposta").

Per eseguire un'analisi di regressione lineare multipla, è importante
che il dataset contenga almeno una variabile dipendente e due o più
variabili indipendenti. Nel dataset "mpg", le variabili "cty" (consumo
di carburante in città) e "hwy" (consumo di carburante in autostrada)
potrebbero essere utilizzate come variabile dipendente, mentre le altre
variabili del dataset (ad esempio, "manufacturer", "displ", "year",
"cyl", "trans", "drv" e "class") potrebbero essere utilizzate come
variabili indipendenti.

Tieni presente che, per eseguire un'analisi di regressione lineare
multipla, è importante che i dati soddisfino alcune assunzioni
statistiche, come la linearità e la varianza costante. Se queste
assunzioni non sono soddisfatte, potrebbe essere necessario trasformare
o selezionare le variabili o utilizzare un modello di regressione
alternativo.

Dato che lo scopo è quello di prevedere il consumo di carburante in base
alle altre caratteristiche del veicolo (ad esempio, il volume del
motore, il tipo di trasmissione o il tipo di carburante utilizzato),
consideriamo l'utilizzo di un modello di regressione. Esistono diverse
varianti di modelli di regressione, come ad esempio la regressione
lineare semplice o la regressione multipla. In questo caso dato che ci
sono molte variabili che andranno ad influire sulla variabile dipendente
utilizzeremo il modello di regressione lineare multiplo.

Per costruire un modello di regressione lineare multipla, è necessario
avere una sufficiente quantità di dati che includano le variabili
indipendenti e la variabile dipendente per ciascuna osservazione. È
quindi possibile utilizzare questi dati per addestrare il modello e
quindi utilizzarlo per fare previsioni sulla variabile dipendente per
nuove osservazioni.

Se si definisce un data frame dfm, contenente n osservazioni delle p + 1
variabili Y, X1, X2, . . . , Xp, allora cov(dfm) e cor(dfm) forniscono
due matrici di dimensioni (p + 1) · (p + 1) i cui elementi solo le
covarianze e le correlazioni tra coppie di variabili. In particolare,
tali matrici sono simmetriche; la matrice delle covarianze contiene
sulla diagonale principale la varianza delle singole colonne del data
frame, mentre la matrice delle correlazioni contiene il numero 1 sulla
diagonale principale. La matrice di correlazione evidenzia tutte le
correlazioni lineari tra le coppie di variabili, ossia misura la forza
del legame di natura lineare esistente tra tutte le coppie di variabili
quantitative.

La funzione seguente ci permette di selezionare solo le variabili
numeriche dunque le variabili di cui andremo a calcolare la correlazione
e la covarianza.

```{r}
dfm <- mpg %>% 
  select_if(is.numeric)

```

### Covarianza

La covarianza (cov) è una misura della correlazione tra due variabili.
Indica se le due variabili cambiano in modo simile o in modo opposto e
in che misura cambiano insieme.

In questo caso la matrice delle covarianze campionarie è

```{r}
cov(dfm)

```

### Correlazione

La correlazione (cor) è una misura della relazione lineare tra due
variabili. Indica se c'è una relazione lineare tra le due variabili e in
che direzione va la relazione. La correlazione può assumere valori
compresi tra -1 e 1. Un valore di correlazione di -1 indica una
relazione negativa perfetta, ovvero le due variabili variano in modo
perfettamente opposto. Un valore di correlazione di 1 indica una
relazione positiva perfetta, ovvero le due variabili variano in modo
perfettamente simile. Un valore di correlazione di 0 indica che non c'è
alcuna relazione lineare tra le due variabili.

In questo caso la matrice delle correlazioni campionarie è

```{r}
cor(dfm)

```

Osservando i valori della covarianza e della correlazione possiamo
notare che le variabili *displ* e *cyl* sono unite da un forte legame
lineare, a differenza delle variabili *displ* e *cty*, *hwy* le quali
sono correlate ma negativamente. La variabile *cyl* è molto correlata
con *displ* ed è correlata negativamente con *cty* e *hwy*. Infine
osservando i valori di *year* possiamo affermare che è correlata in modo
molto debole con le altre variabili.

### Train set e test set

Il train set e il test set vengono utilizzati per valutare l'accuratezza
di un modello di regressione. Il train set viene utilizzato per
addestrare il modello, mentre il test set viene utilizzato per fare
previsioni sulla variabile dipendente e quindi per valutare
l'accuratezza del modello.

```{r}
# Divisione del dataset mpg in un set di training (80%) e un set di test (20%)
partizioni <- createDataPartition(mpg$cty, p = 0.8, list = FALSE)

# Creazione del set di training utilizzando le partizioni ottenute
train_set <- mpg[partizioni, ]

# Creazione del set di test escludendo le partizioni utilizzate per il set di training
test_set <- mpg[-partizioni, ]

```

## Modello di regressione lineare multiplo

Il modello di regressione lineare multipla è un tipo di modello di
regressione che viene utilizzato per prevedere una variabile dipendente
(chiamata anche variabile risposta o target) in base alla relazione con
una o più variabili indipendenti (chiamate anche variabili esplicative o
predictor). Il modello di regressione lineare multipla assume che ci sia
una relazione lineare tra la variabile dipendente e le variabili
indipendenti.

La regressione lineare multipla viene solitamente utilizzata quando ci
sono più di una variabile indipendente che potrebbero influire sulla
variabile dipendente. Ad esempio, potresti utilizzare il modello di
regressione lineare multipla per prevedere il consumo di carburante di
un'automobile (variabile dipendente) in base al volume del motore
(variabile indipendente), al tipo di trasmissione (variabile
indipendente) e al tipo di carburante utilizzato (variabile
indipendente).

```{r}
modello <- lm(cty ~ displ + trans + fl + cyl + drv, data = train_set)
summary(modello)

```

Nel modello di regressione lineare multiplo si è interessati a
sottoporre a verifica l'ipotesi nulla, la quale equivale a dire che la
variabile Xj non influenza la variabile Y.

-   H0 : βj = 0

-   H1 : βj ≠ 0

Se l'ipotesi nulla è vera, la statistica t è :
$$t = \frac{(\hat \beta_{j})}{s . \sqrt(c_{}jj)} \sim t_{n-p-1}$$la
statistica *t* è anche detta *t-value*.

Fissato un livello di significatività $$\alpha$$si determina il valore
soglia $$t_{\alpha/2} $$ tale che:
$$P(|T| > t_{\alpha/2})= P(T < -t_{\alpha/2})+P(T>t_{\alpha/2}) = \alpha$$
Si rifiuta l'ipotesi *nulla* se: $$ |t|>t_{\alpha/2}$$In alternativa, si
calcola il *p-value*: $$P(T > t)=2[1-F(t)]$$ dove *F(t)* è la funzione
di ripartizione della v.c. t-Student.

Con il seguente codice escludiamo le variabili indipendenti che non
influenzano la variabile dipendente, ovvero le variabili *trans* e *fl*.

```{r}
modello_aggiornato <- update(modello, ~ . - trans - fl)
summary(modello_aggiornato)

```

Pertanto possiamo dedurre che sono 3 le variabili che contribuiscono a
definire la variabile dipendete *cty* (miglia per gallone) e sono:
*displ*, *cyl* e *drvf.*

#### Previsioni

Dopo aver stimato il modello di regressione possiamo prevedere i valori
della variabile dipendente su un set di dati che non è stato usato per
addestrare il modello.

```{r}
previsioni <- predict(modello_aggiornato, test_set)

```

### Misure di accuratezza

Per valutare l'accuratezza delle previsioni del modello di regressione
lineare multipla, puossiamo utilizzare diverse metriche di valutazione.
Ad esempio:

-   Errore quadratico medio (MSE): indica la deviazione media quadratica
    tra i valori previsti dal modello e i valori effettivi della
    variabile dipendente. Un valore di MSE piccolo indica che il modello
    fa previsioni accurate, mentre un valore di MSE grande indica che il
    modello fa previsioni meno accurate.

```{r}
MSE <- mean((test_set$cty - previsioni)^2)
MSE

```

-   Coefficiente di determinazione (R²): indica la quantità di varianza
    nella variabile dipendente che viene spiegata dal modello. Un valore
    di R² vicino a 1 indica che il modello spiega in modo accurato la
    varianza nella variabile dipendente, mentre un valore di R² vicino a
    0 indica che il modello non spiega in modo accurato la varianza
    nella variabile dipendente.

```{r}
modello_test <- lm(cty ~ displ + cyl + drv, data = test_set)
summary(modello_test)$r.squared

```

-   Errore assoluto medio (MAE): indica la deviazione media assoluta tra
    i valori previsti dal modello e i valori effettivi della variabile
    dipendente. Un valore di MAE piccolo indica che il modello fa
    previsioni accurate, mentre un valore di MAE grande indica che il
    modello fa previsioni meno accurate.

```{r}
MAE <- mean(abs(test_set$cty - previsioni))
MAE

```

Osservando le misure di accuratezza possiamo affermare che il modello
che abbiamo stimato è un buon modello ovvero è un modello che è in grado
di fare previsioni accurate della variabile dipendente utilizzando le
variabili indipendenti.

# Analisi dei cluster

La cluster analysis (analisi di raggruppamento o dei gruppi) è uno
strumento molto popolare per analizzare dati multivariati non
strutturati. La metodica è composta di diversi algoritmi, ciascuno dei
quali cerca di organizzare un dataset in sottogruppi omogenei, o
"cluster". Non c'è garanzia che si trovi più di un gruppo. Tuttavia,
l'ipotesi fondamentale è che i dati formino un insieme eterogeneo che
dovrebbe suddividersi in gruppi naturali, familiari agli esperti
dell'area analizzata.

I metodi per raggruppare gli item (che siano osservazioni o variabili)
si basano sulla somiglianza (o dissomiglianza) degli item tra loro. Gli
item simili sono quindi trattati come classe o gruppo omogeneo. Gran
parte dell'output di una analisi dei cluster è visiva, con i risultati
visualizzati usando scatterplot, alberi, dendrogrammi, grafici a
silhouette, ed altri strumenti grafici.

Gli algoritmi di clustering (o di partizionamento) sono usualmente
suddivisi tra approcci gerarchici e non gerarchici; gli algoritmi
gerarchici sono a loro volta suddivisi tra metodi agglomerativi e
divisivi. I metodi gerarchici operano aggregando (o separando) tutti gli
item sequenzialmente. I metodi agglomerativi iniziano con ciascun item
che compone il suo proprio cluster; in seguito, vengono aggregati in
successione i cluster più "simili", fino ad ottenere un unico cluster.
Gli algoritmi di clustering divisivi operano in maniera opposta:
iniziano con tutti gli item che appartengono ad un unico cluster;
dividono quindi questo cluster in due cluster separati "internamente
omogenei", e via di seguito; al termine della procedura ciascun item
formerà il "suo" cluster.

In questo manuale presenterò per ora solo esempi sui metodi gerarchici
agglomerativi e non gerarchici, applicati a dati di tipo numerico.

In questa parte del corso vedremo quindi:

1.  Algoritmi di **Clustering Gerarchico (Agglomerativo) (HC)**;

2.  Algoritmi di **Clustering Non-Gerarchici (K-Means) (NHC)**;

## Cluster gerarchico

Il punto di partenza di ogni metodo di Clustering Gerarchico (HC),
quindi, è il calcolo della dissimilarità ("dissomiglianza"; o
"distanza", se vale la disuguaglianza triangolare) di ciascun item
rispetto a tutti gli altri. Quale definizione di distanza usare
(euclidea, Manhattan, Canberra, ecc.), spesso dipende dall'applicazione
specifica o è una scelta soggettiva. Alcune misure di distanza sono
appropriate solo per certi tipi di dato, mentre altre sono state
introdotte per raggruppare variabili (colonne di dati) invece che
osservazioni (righe di dati).

### Distanza e similarità

La somiglianza può essere definita mediante un coefficiente di
similarità $sij = s(Xi,Xj)$ oppure mediante una misura di distanza
$dij = d(Xi,Xj)$ tra due individui $Ii$ e $Ij$ $(i 6= j)$.

Mentre i coefficienti di similarità assumono valori compresi tra 0 e 1,
le misure di distanza possono assumere qualsiasi valore reale maggiore o
uguale a zero.

Una misura di similarità fornisce un valore numerico compreso tra 0 e 1
e permette di definire in modo quantitativo la somiglianza o differenza
tra due individui $Ii$ e $Ij$ , intendendo ovviamente con 0 l'assoluta
assenza e con 1 la massima presenza di somiglianza.

Le misure metriche di somiglianza invece sono soprattutto basate sulle
funzioni distanza tra i vettori delle caratteristiche. Una funzione a
valori reali $d(Xi,Xj)$ è detta funzione distanza se e soltanto se essa
soddisfa le seguenti condizioni:

1.  $d(Xi,Xj) = 0$ se e solo se $Xi = Xj$ , con $Xi$ e $Xj$ in $Ep$;

2.  $d(Xi,Xj) ≥ 0$ per ogni $Xi$ e $Xj$ in $Ep$;

3.  $d(Xi,Xj) = d(Xj ,Xi)$ per ogni $Xi$ e $Xj$ in $Ep$;

4.  $d(Xi,Xj) ≤ d(Xi,Xk) + d(Xk,Xj)$ per ogni $Xi$, $Xj$ e $Xk$ in $Ep$.

La proprietà *1* implica che $Xi$ è a distanza zero da se stesso e che
ogni due punti a distanza nulla debbono essere identici.

La proprietà *2* afferma che la funzione distanza è non negativa.

La proprietà *3* impone la simmetria richiedendo che la distanza tra
$Xi$ e $Xj$ deve essere la stessa della distanza tra $Xj$ e $Xi$.

La proprietà *4*, nota come disuguaglianza triangolare, richiede che la
distanza tra $Xi$ e $Xj$ debba essere sempre minore o uguale della somma
delle distanze di ognuno dei due vettori considerati da qualunque altro
terzo vettore $Xk$.

Uno strumento generale in ***R*** per il calcolo delle distanze è la
funzione `dist()`.

Innanzitutto creiamo una partizione selezionando il *10%* delle
osservazioni del campione totale:

```{r}
p_mpg <- createDataPartition(mpg$class, p = 0.1, list = FALSE)
part_mpg <- mpg[p_mpg, ]

```

Mediante lo scalamento e la standardizzazione si ottengono dei nuovi
dati le cui medie campionarie sono nulle e le varianze campionarie
unitarie. In seguito calcoliamo le matrici delle distanze.

Entrambe le metriche del valore assoluto e del massimo sono
computazionalmente semplici da calcolare con l'unica differenza che la
metrica di Chebycev coinvolge anche una procedura di ordinamento.

```{r}
mpg_std <- scale(part_mpg[,c(3,5,8,9)],center = TRUE,scale = TRUE)

## Metrica Euclidea
d <- dist(mpg_std, method = "euclidean",diag = TRUE, upper = TRUE)

## Metrica di Manhattan
d <- dist(mpg_std, method = "euclidean",diag = TRUE, upper = TRUE)

## Metrica di Chebycev
d <- dist(mpg_std, method = "euclidean",diag = TRUE, upper = TRUE)

```

### Metodi gerarchici

I metodi gerarchici di clustering eseguono una sequenza ordinata di
operazioni della stessa natura. Tali metodi sono distinti in
*agglomerativi* che procedono per aggregazioni successive delle unità
partendo da n gruppi formati da un singolo individuo e *divisivi* che
partono da un solo gruppo formato da tutte le unità e procedono a
divisioni successive fino a giungere a gruppi formati da una sola unità.

I metodi gerarchici hanno due vantaggi: quello di fornire una visione
completa dell'insieme in termini di distanza (o di coefficienti di
similarità) seppure condizionata dalla scelta del metodo seguito e
quello di non comportare la scelta a priori del numero di cluster oppure
la scelta a priori di parametri per la determinazione automatica del
loro numero. Invece, uno svantaggio di tali metodi è che essi non
consentono di riallocare gli individui che sono stati già classificati
ad un livello precedente dell'analisi.

I metodi *non gerarchici* di clustering consentono, a differenza delle
tecniche di tipo gerarchico, di riallocare gli individui già
classificati ad un livello precedente dell'analisi. A differenza dei
metodi gerarchici, l'obiettivo finale dei metodi non gerarchici è quello
di ottenere un'unica partizione degli n individui di partenza in
cluster. In molti metodi non gerarchici di clustering si assume che il
numero di cluster in cui suddividere l'insieme totale degli $n$
individui sia fissato a priori dal ricercatore, mentre in altri tale
numero è determinato nel corso dell'analisi.

L'obiettivo finale dei metodi gerarchici non \`e quello di ottenere una
singola partizione degli n individui di partenza, ma di ottenere una
sequenza di partizioni che possono essere rappresentate graficamente
mediante una struttura ad albero, detta dendrogramma, nella quale
sull'insieme delle ordinate sono riportati i livelli di distanza mentre
sull'asse delle ascisse sono riportati i singoli individui. Ad ogni
livello di distanza corrisponde una partizione, mentre ad ogni
partizione corrispondono infiniti livelli di distanza compresi tra
quelli che individuano due successive unioni o divisioni.

In ogni modo, dopo aver ottenuto la matrice di dissimilarità, per
applicare un clustering gerarchico si deve specificare come si vuole
siano calcolate le distanze tra gruppi di osservazioni (cluster) durante
le iterazioni dell'algoritmo di clustering stesso. Anche in questo caso
ci sono diverse scelte da fare. Tra queste, c'è la scelta del cosiddetto
metodo di linkage (legame):

single linkage ==\> (legame singolo) la distanza tra due gruppi è
definita come il valore più piccolo delle distanze tra gli item dei due
gruppi;

complete linkage ==\> (legame completo) la distanza tra due gruppi è
definita come il valore più grande di distanza tra gli item dei due
gruppi;

average linkage ==\> (legame medio) questo è un compromesso tra i due
precedenti approcci, ottenuto come la media delle corrispondenti
distanze.

centroid linkage ==\> (legame del centroide) In questo metodo la
distanza tra i gruppi è definita come la distanza tra i centroidi, ossia
tra le medie campionarie calcolate sugli individui appartenenti ai due
gruppi.

median linkage ==\> (legame della mediana) Il metodo della mediana è
simile a quello del centroide, con la differenza che la procedura è
indipendente dalla numerosità dei cluster. Infatti, quando due gruppi si
aggregano, il nuovo centroide è calcolato come la semisomma dei due
centroidi precedenti.

Nei metodi agglomerativi del legame singolo, del legame completo e del
legame medio si può utilizzare una qualsiasi misura di distanza. Invece,
nel metodo del centroide e nel metodo della mediana si considera la
distanza euclidea e si lavora con una matrice D(2) che contiene i
quadrati delle singole distanze euclidee.

```{r}
util_sl <- hclust(d, method = "single")
util_cl <- hclust(d, method = "complete")
util_al <- hclust(d, method = "average")
util_cent <- hclust(d, method = "centroid")

```

Una volta applicati i metodi di clustering come sopra, l'approccio
grafico standard per rappresentare la soluzione di un algoritmo
gerarchico agglomerativo è quello del dendrogramma, che rappresenta la
sequenza di fusioni successive dei diversi gruppi, nonché le distanze su
cui sono state prodotte le aggregazioni Un dendrogramma in ***R*** è
disponibile con il metodo `plot` per gli oggetti di classe (tipo)
`hclust`:

```{r}
op <- par(mfrow = c(2, 2))
plot(util_sl, labels = part_mpg$class, cex = .7,
        main = "Utilities data (single linkage)", xlab = "Utilities")
plot(util_cl, labels = part_mpg$class, cex = .7,
        main = "Utilities data (complete linkage)", xlab = "Utilities")
plot(util_al, labels = part_mpg$class, cex = .7,
        main = "Utilities data (average linkage)", xlab = "Utilities")
plot(util_cent, labels = part_mpg$class, cex = .7,
        main = "Utilities data (Ward)", xlab = "Utilities")

```

Come si vede, il metodo del legame singolo tende a creare grandi gruppi
aggiungendo in successione item a gruppi già creati (il cosiddetto
effetto di "concatenazione"). Gli altri tre metodi ritornano invece
risultati simili tra loro, con cluster più strutturati.

Il dendrogramma costruito con questo metodo ha i rami molto più lunghi
rispetto al dendrogramma ottenuto con il metodo del legame singolo
poiché i gruppi si formano a livelli di distanza maggiori.

Si nota che con il metodo del legame medio la suddivisione in cluster
non cambia rispetto al caso del legame singolo e del legame completo, ma
variano alcuni livelli di distanza relativi alle aggregazioni;

Una volta scelto il numero di gruppi (più avanti vedremo anche questo
aspetto), si può usare la funzione `cutree()` per ottenere a quale
gruppo (cluster) appartiene ciascun item:

```{r}
util_cl_m <- cutree(util_cl, k = 5)
# Calcola i valori di silhouette
sil <- silhouette(util_cl_m, d)
# disegna il silhouette
plot(sil)

```

Come si vede, il parametro `k` definisce il numero di gruppi che si
vogliono ottenere.\
L'appartenenza ai cluster, che otteniamo dalla chiamata della funzione
`cutree()`, è utile per produrre una profilazione dei cluster, cioè per
fornire una descrizione più dettagliata delle caratteristiche principali
dei cluster identificati. Un ulteriore approccio grafico per ricavare
l'appartenenza ai cluster è dato dalla funzione `rect.hclust()`; questa
aggiunge al dendrogramma dei rettangoli che mostrano i cluster
identificati. Il grafico che segue identifica 5 gruppi con legame
completo:

```{r}
plot(util_cl)
rect.hclust(util_cl, k = 5)

```

Il grafico qui sopra mostra i valori di silhouette per i singoli item.
Ogni barra orizzontale rappresenta il valore di silhouette (sisi) di un
item all'interno del suo cluster assegnato. sisi è sempre compreso tra
-1 e 1. Un valore di sisi prossimo a 1 indica che l'item è assegnato ad
un cluster che "lo rappresenta" molto bene. Un valore di sisi prossimo a
-1 indica che l'item ii-mo è più simile ad item che appartengono ad
altri cluster vicini. Un sisi prossimo a zero indica che l'item ii-mo si
trova in una posizione sostanzialmente intermedia (cioè, sul bordo) tra
due o più cluster.

### Screeplot

Un metodo euristico per scegliere una buona partizione del dendrogramma
considera una procedura empirica consistente nel costruire un grafico,
detto screeplot. In esso si pongono sull'asse delle ordinate i numeri di
gruppi ottenibili con il metodo gerarchico e sull'asse delle ascisse le
distanze a cui avvengono le successive aggregazioni tra i gruppi. Se nel
passaggio da k gruppi a k − 1 gruppi si registra un forte incremento
della distanza di aggregazione è consigliabile tagliare il dendrogramma
in k gruppi.

Occorre infine sottolineare che lo screeplot è un metodo empirico che
realizza un grafico basato sulle altezze a cui sono avvenute le
aggregazioni in un metodo gerarchico e quindi non sempre fornisce il
numero adeguato di cluster in cui suddividere gli individui. Inoltre, è
preferibile non utilizzare lo screeplot nel metodo del centroide e della
mediana. Per la suddivisione in cluster in un metodo gerarchico è sempre
preferibile ricorrere alle misure di non omogeneità statistiche di cui
si serviranno, come vedremo, anche i metodi non gerarchici.

### Cluster non gerarchico

L'obiettivo dei metodi non gerarchici \`e quello di ottenere un'unica
partizione degli n individui di partenza in cluster. A differenza dei
metodi gerarchici, in tali tecniche è consentito riallocare gli
individui già classificati ad un livello precedente dell'analisi.

Gli algoritmi di tipo non gerarchico procedono, data una prima
partizione, a riallocare gli individui nel gruppo con centroide più
vicino, fino a che per nessun individuo si verifica che sia minima la
distanza rispetto al centroide di un gruppo diverso da quello a cui esso
appartiene. Il metodo più utilizzato prende il nome di k-means ed è
dovuto a Hartigan e Wong1. Tale metodo richiede che il numero di cluster
sia specificato a priori e fornisce in output un'unica partizione.

I vantaggi del metodo k-means sono la velocità di esecuzione dei calcoli
e l'estrema libertà che viene lasciata agli individui di raggrupparsi e
allontanarsi. Uno svantaggio è invece che la classificazione finale può
essere influenzata dalla scelta iniziale dei k vettori delle
caratteristiche come punti di riferimento, dall'ordine in cui sono presi
tali vettori e naturalmente dalle proprietà geometriche dei vettori
delle misure.

Dato un numero preassegnato *k* di gruppi, i metodi di clustering non
gerarchico (NHC) cercano la paritizione dei dati ("righe" o item) in *k*
cluster in maniera tale che gli item entro ciascun cluster o gruppo
siano quanto possibile simili tra loro, mentre gli item appartenenti a
cluster diversi siano quanto possibile diversi.

Un possibile approccio per ottenere questo risultato potrebbe passare
attraverso l'elencazione di tutti i possibili raggruppamenti in *k*
gruppi costruibili con gli item e quindi scegliere come migliore
soluzione il raggruppamento che ottimizza un qualche criterio
predefinito. Sfortunatamente, un tale approccio diventerebbe rapidamente
inapplicabile, specialmente per grandi dataset, poiché richiederebbe una
quantità enorme di tempo macchina e di spazio di memoria. Di conseguenza
tutte le tecniche di clustering disponibili sono iterative e operano
solo su un numero molto ristretto di enumerazioni.

Tra i molti algoritmi di clustering non gerarchici sviluppati finora, il
più popolare è il K-means. Nella sua implementazione fondamentale,
l'algoritmo K-means inizia assegnando gli item a uno dei *k* cluster
predeterminati e quindi calcolando i *k* centroidi di gruppo, oppure
pre-specificando i *k* centroidi di gruppo. I centroidi pre-specificati
possono essere item selezionati casualmente oppure possono essere
ottenuti "tagliando" un dendrogramma ad una altezza appropriata.\
In seguito, tramite una procedura iterativa, l'algoritmo cerca di
minimizzare la somma dei quadrati entro gruppi (WGSS) su tutte le
variabili, riassegnando ("spostando") gli item sui diversi cluster. La
procedura si ferma quando non si ottengono miglioramenti di WGSS con
ulteriori riassegnamenti.

La soluzione così ottenuta può non essere unica: l'algoritmo potrebbe
trovare solo un minimo locale di WGSS. Si suggerisce pertanto di
lanciare più volte l'algoritmo con assegnazioni casuali iniziali
differenti degli item ai *k* cluster (oppure selezionando casualmente i
*k* centroidi iniziali), così da trovare un probabile minimo globale di
WGSS e, quindi, la migliore soluzione di clustering basata su *k*
cluster.

Se calcoliamo prima le varianze delle variabili troviamo i seguenti:

```{r}
sapply(part_mpg[,c(3,5,8,9)], var)

```

Le varianze sono molto diverse, e l'uso del K-means sui dati grezzi
potrebbe fornire risultati poco utili perché "dominati" dalle
caratteristiche più variabili. Come nel caso del clustering gerarchico,
quindi, dovremmo normalizzare i dati in qualche modo; in questo caso
scegliamo di normalizzare ogni variabile dividendone i valori per il
rispettivo range.

```{r}
# calcola il range di ogni variabile
rge <- sapply(part_mpg[,c(3,5,8,9)], function(x) diff(range(x)))
# Divide ogni valore di ogni variabile per il suo range
mpg_s <- sweep(x = part_mpg[,c(3,5,8,9)], MARGIN = 2, STATS = rge, FUN = "/")
# Calcola la varianza di tutte le variabili "normalizzate"
sapply(mpg_s, var)

```

Adesso sono molto più confrontabili. Possiamo quindi procedere con il
clustering dei dati normalizzati. Dapprima tracciamo i valori di WGSS
per soluzioni di raggruppamento con un numero di cluster che varia tra 1
e 8 per vedere se possiamo ottenere indicazioni sul numero di gruppi
(che usualmente non è noto a priori). Il grafico può essere ottenuto
come segue:

```{r}
k_max <- 8
wss <- sapply(1:k_max,
              function(k,data) kmeans(data, centers = k)$tot.withinss,
              data=mpg_s)
ggp <- ggplot(data = data.frame(x=1:k_max, y=wss), mapping = aes(x=x,y=y)) +
  geom_point(colour = "red") +
  geom_line(colour = "blue") +
  xlab("Numero di gruppi") + 
  ylab("Somma dei quadrati entro gruppi (WGSS)")
print(ggp)

```

Mano a mano che il numero di gruppi aumenta, la somma dei quadrati
necessariamente si riduce; se si presenta un "gomito" nel grafico,
questo potrebbe essere indicativo della soluzione più utile che
l'analista dovrebbe osservare in dettaglio. Nel nostro caso un possibile
"gomito" (anche se non particolarmente evidente) potrebbe essere nella
soluzione con tre gruppi.\
Analizzeremo quindi questa soluzione. Le medie di gruppo per questa
soluzione sono calcolabili tramite:

```{r}
km_3 <- kmeans(mpg_s, centers = 3)
km_3$centers * rge

```

Volendo, possiamo provare l'indice di Calinski-Harabasz anche con il
clustering non gerarchico, per trovare il numero "ottimale" di cluster:

```{r}
require(clusterSim)
minC <- 2
maxC <- 10
res <- sapply(minC:maxC,
              function(nc, data) index.G1(data, kmeans(data,centers = nc)$cluster),
              data = mpg_s)
ggp <- ggplot(data=data.frame(x=2:(length(res)+1), y= res), mapping = aes(x=x,y=y)) + 
  geom_point() + 
  geom_line() +
  xlab("Numero di cluster") +
  ylab("Statistica pseudo-F di Calinski-Harabasz")
print(ggp)

```

Questo indice fornisce anche in questo caso una chiara indicazione che
il numero di cluster da scegliere debba essere uguale a 3.

Per visualizzare i cluster, possiamo utilizzare il pacchetto ggplot2 per
creare un grafico a dispersione dei dati con i punti colorati in base al
cluster a cui appartengono:

```{r}
ggplot(part_mpg[,c(3,5,8,9)], aes(x = displ, y = cty, color = km_3$cluster)) +
  geom_point()

```

In conclusione, i metodi gerarchici presentano un evidente vantaggio dal
punto di vista computazionale rispetto ai metodi di enumerazione
completa; tuttavia risultano sensibili a vettori delle caratteristiche
anomali e non consentono di modificare la configurazione raggiunta,
ossia una volta che un individuo è stato attribuito ad un cluster
permane al suo interno per sempre. I metodi non gerarchici non
presentano questo problema poiché consentono di riallocare gli individui
precedentemente classificati, ma richiedono una scelta opportuna della
configurazione iniziale.
